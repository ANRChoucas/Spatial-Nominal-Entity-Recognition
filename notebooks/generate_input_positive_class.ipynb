{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate input for positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the lexicon from terms embedded in ENE from Perdido XML-TEI\n",
    "\n",
    "\\<rs type=\"place\">\\<term type=\"place\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_occurrences_from_ene(filename):\n",
    "    words = []\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            tree = etree.parse(filename)\n",
    "            for term in tree.xpath('.//rs[@type=\"place\" and @subtype=\"ene\"]/term[@type=\"place\"]'):\n",
    "                phrase = ''\n",
    "                tokens = term.xpath('.//w[@pos=\"N\" or @pos=\"PREPDET\" or @pos=\"PREP\" or @pos=\"DET\"]')\n",
    "                for i, w in enumerate(tokens):\n",
    "                    if len(w.text) > 1:\n",
    "                        if ('DET' not in w.get('pos') and 'PREP' not in w.get('pos')):\n",
    "                            phrase += w.text.lower() + ' '\n",
    "                        if ('DET' in w.get('pos') or 'PREP' in w.get('pos')) and (i > 0 and i < len(tokens)-1):\n",
    "                            phrase += w.text.lower() + ' '\n",
    "                words.append(phrase.strip())\n",
    "        except:\n",
    "            pass\n",
    "    return words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Corpus Traitement Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Choucas/Perdido'\n",
    "output_filename = 'Traitement_Texte_pivot_lexicon'\n",
    "\n",
    "words = []\n",
    "for doc in tqdm(sorted(os.listdir(path))):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') # version Traitements_Texte\n",
    "    #filename = os.path.join(path, doc[:-4]+'.xml') # version visorando\n",
    "    words.extend(get_term_occurrences_from_ene(filename))\n",
    "\n",
    "# list to dict with frequency\n",
    "frequency_dict_geo = {value: words.count(value) for value in words}\n",
    "print('Size of the lexicon', len(frequency_dict_geo))\n",
    "\n",
    "#save the dict in file\n",
    "with open(output_filename + '.json', 'w') as fp:\n",
    "    json.dump(frequency_dict_geo, fp, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Corpus Visorando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Visorando/Perdido'\n",
    "output_filename = 'Visorando_pivot_lexicon'\n",
    "\n",
    "words = []\n",
    "for doc in tqdm(sorted(os.listdir(path))):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') # version Traitements_Texte\n",
    "    #filename = os.path.join(path, doc[:-4]+'.xml') # version visorando\n",
    "    words.extend(get_term_occurrences_from_ene(filename))\n",
    "\n",
    "# list to dict with frequency\n",
    "frequency_dict_geo = {value: words.count(value) for value in words}\n",
    "print('Size of the lexicon', len(frequency_dict_geo))\n",
    "\n",
    "#save the dict in file\n",
    "with open(output_filename + '.json', 'w') as fp:\n",
    "    json.dump(frequency_dict_geo, fp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in frequency_dict_geo.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dict from file\n",
    "with open(output_filename + '.json') as fp:\n",
    "    frequency_dict_geo = json.load(fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Find occurrences of the lexicon in the corpus (outside ENE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_wt_term_outside_ene(filename):\n",
    "    json_content = []\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            tree = etree.parse(filename)\n",
    "            tokens = tree.xpath('.//w')\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.text in frequency_dict_geo:\n",
    "                    line = {'num':ngram_id, 'class':'1', 'id_phrase':'0','pivot':token.text,'occurrence': '0', 'url':os.path.join(doc, doc+'.xml')}\n",
    "                    phrase = []\n",
    "                    for j in range(3,0,-1):\n",
    "                        try:\n",
    "                            words = {'word':tokens[i-j].text, 'POS':tokens[i-j].get('pos'), 'lemma':tokens[i-j].get('lemma')}\n",
    "                        except IndexError:\n",
    "                            words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                        phrase.append(words)\n",
    "                    phrase.append({'word':token.text, 'POS':token.get('pos') + '+LS', 'lemma':token.get('lemma')})\n",
    "                    for j in range(1,4):\n",
    "                        try:\n",
    "                            words = {'word':tokens[i+j].text, 'POS':tokens[i+j].get('pos'), 'lemma':tokens[i+j].get('lemma')}\n",
    "                        except IndexError:\n",
    "                            words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                        phrase.append(words)\n",
    "                    line['phrase'] = phrase\n",
    "                    try:\n",
    "                        print(tokens[i-3].text, tokens[i-2].text , tokens[i-1].text , '[', token.text, ']', tokens[i+1].text, tokens[i+2].text, tokens[i+3].text)\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    ngram_id += 1\n",
    "                    json_content.append(line)\n",
    "        except :\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Corpus Traitement text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_content = []\n",
    "ngram_id = 1\n",
    "for doc in sorted(os.listdir(path)):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') # version Traitements_Texte\n",
    "    json_content.extend(get_ngrams_wt_term_outside_ene(filename))\n",
    "\n",
    "print('nnumber of ngram',len(json_content))\n",
    "\n",
    "name = 'Traitement_Texte_class1'\n",
    "with open(name + \".json\", \"w\") as outfile:\n",
    "    json.dump(json_content,outfile, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Corpus Visorando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_content = []\n",
    "ngram_id = 1\n",
    "for doc in sorted(os.listdir(path)):\n",
    "    filename = os.path.join(path, doc[:-4]+'.xml') # version visorando\n",
    "    json_content.extend(get_ngrams_wt_term_outside_ene(filename))\n",
    "                            \n",
    "print('nnumber of ngram',len(json_content))\n",
    "\n",
    "name = 'Visorando_class1'\n",
    "with open(name + \".json\", \"w\") as outfile:\n",
    "    json.dump(json_content,outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of terms in ENE not categorized by Perdido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for doc in tqdm(sorted(os.listdir(path))):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') \n",
    "    if os.path.exists(filename):\n",
    "        tree = etree.parse(filename)\n",
    "\n",
    "        for term in tree.xpath('.//rs[@type=\"unknown\" and @subtype=\"ene\"]/term[@type=\"unknown\"]'):\n",
    "            phrase = ''\n",
    "            for w in term.xpath('.//w[@pos=\"N\" or @pos=\"PREPDET\" or @pos=\"PREP\"]'):\n",
    "                phrase += w.text.lower() + ' '\n",
    "                #print(w.text, end=' ')\n",
    "            words.append(phrase.strip())\n",
    "            #print()\n",
    "\n",
    "# list to dict with frequency\n",
    "frequency_dict_unknown = {value: words.count(value) for value in words}\n",
    "print('Size of the lexicon', len(frequency_dict_unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files with rs in term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Visorando/Perdido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for doc in tqdm(sorted(os.listdir(path))):\n",
    "    filename = os.path.join(path, doc+'.xml') \n",
    "    if os.path.exists(filename):\n",
    "        tree = etree.parse(filename)\n",
    "\n",
    "\n",
    "        for term in tree.xpath('.//term//rs'):\n",
    "            print(filename)\n",
    "            phrase = ''\n",
    "            for w in term.xpath('.//w'):\n",
    "                phrase += w.text.lower() + ' '\n",
    "        #print(phrase, end=' ')\n",
    "        #print()\n",
    "            \n",
    "            #print()\n",
    "\n",
    "        # list to dict with frequency\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
