{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate input for negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import json\n",
    "import random \n",
    "import string\n",
    "from perdido.geoparser import Geoparser\n",
    "\n",
    "\n",
    "from utils_functions import load_lexicon, load_edda_dataframe, run_perdido, get_term_occurrences_from_ene, segment_sentences, get_ngrams_wt_term_outside_ene"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load lexicon\n",
    "\n",
    "The lexicon is created with the notebook `generate_input_positive_class.ipynb`. (TODO: make an independant script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_filename =  os.path.join('output', 'Traitement_Texte_pivot_lexicon.json')\n",
    "\n",
    "frequency_dict_geo_tt = load_lexicon(lexicon_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frequency_dict_geo_tt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load EDdA dataset\n",
    "\n",
    "Load the csv of EDdA dataset as a dataframe. Article from the Encyclop√©die will be used to generate negative ngrams with words from the lexicon as pivot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edda_dataset_path = '/Users/lmoncla/Nextcloud-LIRIS/GEODE/GEODE - Partage consortium/Corpus/EDdA/EDdA_dataset_articles_superdomainBERT_230327.tsv'\n",
    "data = load_edda_dataframe(edda_dataset_path, 'Philosophie')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences_per_article = data['content'].apply(segment_sentences)\n",
    "df = data.head()\n",
    "sentences_per_article = df['content'].apply(segment_sentences)\n",
    "\n",
    "all_sentences = [sentence for sentences in sentences_per_article for sentence in sentences]\n",
    "\n",
    "# Print the list of sentences\n",
    "for sentence in all_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# build batches of sentences\n",
    "batch_size = 50\n",
    "batches = [all_sentences[i:i + batch_size] for i in range(0, len(all_sentences), batch_size)]\n",
    "\n",
    "# turn each batch into a concatenate string\n",
    "batch_strings = [' '.join(batch) for batch in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './output/Philosophie'\n",
    "geoparser = Geoparser(version='Standard', sources=['wiki_gaz'])\n",
    "\n",
    "for batch in tqdm(batch_strings):\n",
    "    # generate a random string of 10 characters\n",
    "    filename = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n",
    "    doc = run_perdido(batch, geoparser)\n",
    "    try:\n",
    "        doc.to_xml(os.path.join(output_dir, filename + '.xml'))\n",
    "    except:\n",
    "        print('Error', filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Corpus Traitement Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Choucas/Perdido'\n",
    "output_filename = 'Traitement_Texte_pivot_lexicon'\n",
    "\n",
    "words_TT = []\n",
    "for doc in tqdm(sorted(os.listdir(path))):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') # version Traitements_Texte\n",
    "    words_TT.extend(get_term_occurrences_from_ene(filename))\n",
    "\n",
    "# list to dict with frequency\n",
    "frequency_dict_geo_TT = {value: words_TT.count(value) for value in words_TT}\n",
    "print('Size of the lexicon', len(frequency_dict_geo_TT))\n",
    "\n",
    "#save the dict in file\n",
    "with open(output_filename + '.json', 'w') as fp:\n",
    "    json.dump(frequency_dict_geo_TT, fp, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Corpus Visorando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Visorando/Perdido'\n",
    "output_filename = 'Visorando_pivot_lexicon'\n",
    "\n",
    "words_viso = []\n",
    "for doc in tqdm(sorted(os.listdir(path))):\n",
    "    filename = os.path.join(path, doc[:-4]+'.xml') # version visorando\n",
    "    words_viso.extend(get_term_occurrences_from_ene(filename))\n",
    "\n",
    "# list to dict with frequency\n",
    "frequency_dict_geo_viso = {value: words_viso.count(value) for value in words_viso}\n",
    "print('Size of the lexicon', len(frequency_dict_geo_viso))\n",
    "\n",
    "#save the dict in file\n",
    "with open(output_filename + '.json', 'w') as fp:\n",
    "    json.dump(frequency_dict_geo_viso, fp, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Lexique TT - Viso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [item for item in frequency_dict_geo_viso.keys() if item not in frequency_dict_geo_TT.keys()]\n",
    "print(813-190)\n",
    "print(len(words), words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Lexicons preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in frequency_dict_geo_viso.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dict from file\n",
    "# only necessery if you want to run the nex section and not the previous one (adapt output_filename)\n",
    "with open(output_filename + '.json') as fp:\n",
    "    frequency_dict_geo_viso = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Find occurrences of the lexicon in the corpus (outside ENE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_wt_term_outside_ene(filename, frequency_dict_geo, ngram_id):\n",
    "    json_content = []\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            \n",
    "            tree = etree.parse(filename)\n",
    "            tokens = tree.xpath('.//w')\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.text in frequency_dict_geo:\n",
    "                    line = {'num':ngram_id, 'class':'1', 'id_phrase':'0','pivot':token.text,'occurrence': '0', 'url':os.path.join(doc, doc+'.xml')}\n",
    "                    phrase = []\n",
    "                    for j in range(3,0,-1):\n",
    "                        try:\n",
    "                            words = {'word':tokens[i-j].text, 'POS':tokens[i-j].get('pos'), 'lemma':tokens[i-j].get('lemma')}\n",
    "                        except IndexError:\n",
    "                            words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                        phrase.append(words)\n",
    "                    phrase.append({'word':token.text, 'POS':token.get('pos') + '+LS', 'lemma':token.get('lemma')})\n",
    "                    for j in range(1,4):\n",
    "                        try:\n",
    "                            words = {'word':tokens[i+j].text, 'POS':tokens[i+j].get('pos'), 'lemma':tokens[i+j].get('lemma')}\n",
    "                        except IndexError:\n",
    "                            words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                        phrase.append(words)\n",
    "                    line['phrase'] = phrase\n",
    "                    try:\n",
    "                        print(tokens[i-3].text, tokens[i-2].text , tokens[i-1].text , '[', token.text, ']', tokens[i+1].text, tokens[i+2].text, tokens[i+3].text)\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    ngram_id += 1\n",
    "                    json_content.append(line)\n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return json_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Corpus Traitement text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Choucas/Perdido'\n",
    "\n",
    "#lexicon = frequency_dict_geo_TT\n",
    "lexicon = frequency_dict_geo_viso\n",
    "\n",
    "json_content = []\n",
    "ngram_id = 1\n",
    "for doc in sorted(os.listdir(path)):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') # version Traitements_Texte\n",
    "    json_content.extend(get_ngrams_wt_term_outside_ene(filename, lexicon, ngram_id))\n",
    "\n",
    "print('number of ngram',len(json_content))\n",
    "\n",
    "name = 'Traitement_Texte_class1'\n",
    "with open(name + \".json\", \"w\") as outfile:\n",
    "    json.dump(json_content,outfile, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Corpus Visorando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Visorando/Perdido'\n",
    "json_content = []\n",
    "ngram_id = 1\n",
    "for doc in sorted(os.listdir(path)):\n",
    "    filename = os.path.join(path, doc[:-4]+'.xml') # version visorando\n",
    "    json_content.extend(get_ngrams_wt_term_outside_ene(filename, frequency_dict_geo_viso, ngram_id))\n",
    "                            \n",
    "print('number of ngram',len(json_content))\n",
    "\n",
    "name = 'Visorando_class1'\n",
    "with open(name + \".json\", \"w\") as outfile:\n",
    "    json.dump(json_content,outfile, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TT - Viso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Choucas/Perdido'\n",
    "\n",
    "json_content = []\n",
    "ngram_id = 1\n",
    "for doc in sorted(os.listdir(path)):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') # version Traitements_Texte\n",
    "    json_content.extend(get_ngrams_wt_term_outside_ene(filename, words, ngram_id))\n",
    "\n",
    "print('number of ngram',len(json_content))\n",
    "\n",
    "name = 'TT-viso_class1'\n",
    "with open(name + \".json\", \"w\") as outfile:\n",
    "    json.dump(json_content,outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = set([ngram['pivot']for ngram in json_content])\n",
    "len(l)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Tests with ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_wt_term_outside_ene(filename, frequency_dict_geo, ngram_id, position='center'):\n",
    "    json_content = []\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            tree = etree.parse(filename)\n",
    "            tokens = tree.xpath('.//w')\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.text in frequency_dict_geo:\n",
    "                    line = {'num':ngram_id, 'class':'1', 'id_phrase':'0','pivot':token.text,'occurrence': '0', 'url':os.path.join(doc, doc+'.xml')}\n",
    "                    phrase = []\n",
    "                    if position == 'center':\n",
    "                        for j in range(3,0,-1):\n",
    "                            try:\n",
    "                                words = {'word':tokens[i-j].text, 'POS':tokens[i-j].get('pos'), 'lemma':tokens[i-j].get('lemma')}\n",
    "                            except IndexError:\n",
    "                                words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                            phrase.append(words)\n",
    "                        phrase.append({'word':token.text, 'POS':token.get('pos') + '+LS', 'lemma':token.get('lemma')})\n",
    "                        for j in range(1,4):\n",
    "                            try:\n",
    "                                words = {'word':tokens[i+j].text, 'POS':tokens[i+j].get('pos'), 'lemma':tokens[i+j].get('lemma')}\n",
    "                            except IndexError:\n",
    "                                words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                            phrase.append(words)\n",
    "                        try:\n",
    "                            print(tokens[i-3].text, tokens[i-2].text , tokens[i-1].text , '[', token.text, ']', tokens[i+1].text, tokens[i+2].text, tokens[i+3].text)\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "                    if position == 'left':\n",
    "                        phrase.append({'word':token.text, 'POS':token.get('pos') + '+LS', 'lemma':token.get('lemma')})\n",
    "                        for j in range(1,7):\n",
    "                            try:\n",
    "                                words = {'word':tokens[i+j].text, 'POS':tokens[i+j].get('pos'), 'lemma':tokens[i+j].get('lemma')}\n",
    "                            except IndexError:\n",
    "                                words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                            phrase.append(words)\n",
    "                        try:\n",
    "                            print('[', token.text, ']', tokens[i+1].text, tokens[i+2].text, tokens[i+3].text, tokens[i+4].text, tokens[i+5].text, tokens[i+6].text)\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "                    if position == 'right':\n",
    "                        for j in range(6,0,-1):\n",
    "                            try:\n",
    "                                words = {'word':tokens[i-j].text, 'POS':tokens[j].get('pos'), 'lemma':tokens[i-j].get('lemma')}\n",
    "                            except IndexError:\n",
    "                                words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                            phrase.append(words)\n",
    "                        phrase.append({'word':token.text, 'POS':token.get('pos') + '+LS', 'lemma':token.get('lemma')})\n",
    "                        try:\n",
    "                            print(tokens[i-6].text, tokens[i-5].text, tokens[i-4].text, tokens[i-3].text, tokens[i-2].text, tokens[i-1].text, '[', token.text, ']')\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "                    line['phrase'] = phrase\n",
    "                    \n",
    "                    ngram_id += 1\n",
    "                    json_content.append(line)\n",
    "        except:\n",
    "            print(\"pass\")\n",
    "            pass\n",
    "    \n",
    "    return json_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_wt_term_outside_ene(filename, frequency_dict_geo, ngram_id, position=4, ngram_size=7):\n",
    "    json_content = []\n",
    "    print_content = ''\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            tree = etree.parse(filename)\n",
    "            tokens = tree.xpath('.//w')\n",
    "            for i, token in enumerate(tokens):\n",
    "                print_content = ''\n",
    "                if token.text in frequency_dict_geo:\n",
    "                    line = {'num':ngram_id, 'class':'1', 'id_phrase':'0','pivot':token.text,'occurrence': '0', 'url':os.path.join(doc, doc+'.xml')}\n",
    "                    phrase = []\n",
    "                    for j in range(position-1, 0, -1):\n",
    "                        try:\n",
    "                            words = {'word':tokens[i-j].text, 'POS':tokens[i-j].get('pos'), 'lemma':tokens[i-j].get('lemma')}\n",
    "                            print_content += tokens[i-j].text + ' '\n",
    "                        except IndexError:\n",
    "                            words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                            print_content += '_ '\n",
    "                        phrase.append(words)\n",
    "                        \n",
    "                    phrase.append({'word':token.text, 'POS':token.get('pos') + '+LS', 'lemma':token.get('lemma')})\n",
    "                    print_content += '[ ' + token.text + ' ] '\n",
    "                    for j in range(1, ngram_size+1-position):\n",
    "                        try:\n",
    "                            words = {'word':tokens[i+j].text, 'POS':tokens[i-j].get('pos'), 'lemma':tokens[i-j].get('lemma')}\n",
    "                            print_content += tokens[i+j].text + ' '\n",
    "                        except IndexError:\n",
    "                            words = {'word':'_', 'POS':'_', 'lemma':'_'}\n",
    "                            print_content += '_ '\n",
    "                        phrase.append(words)\n",
    "                        \n",
    "                    print(print_content)\n",
    "                    line['phrase'] = phrase\n",
    "                    \n",
    "                    ngram_id += 1\n",
    "                    json_content.append(line)\n",
    "        except:\n",
    "            print(\"pass\")\n",
    "            pass\n",
    "    \n",
    "    return json_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Choucas/Perdido'\n",
    "\n",
    "lexicon = frequency_dict_geo_TT\n",
    "#lexicon = frequency_dict_geo_viso\n",
    "position = 7\n",
    "json_content = []\n",
    "ngram_id = 1\n",
    "for doc in sorted(os.listdir(path)):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') # version Traitements_Texte\n",
    "    json_content.extend(get_ngrams_wt_term_outside_ene(filename, lexicon, ngram_id, position=position))\n",
    "\n",
    "print('number of ngram',len(json_content))\n",
    "\n",
    "name = 'Traitement_Texte_class1_position'+str(position)\n",
    "with open(name + \".json\", \"w\") as outfile:\n",
    "    json.dump(json_content,outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of terms in ENE not categorized by Perdido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for doc in tqdm(sorted(os.listdir(path))):\n",
    "    filename = os.path.join(path, doc, doc+'.xml') \n",
    "    if os.path.exists(filename):\n",
    "        tree = etree.parse(filename)\n",
    "\n",
    "        for term in tree.xpath('.//rs[@type=\"unknown\" and @subtype=\"ene\"]/term[@type=\"unknown\"]'):\n",
    "            phrase = ''\n",
    "            for w in term.xpath('.//w[@pos=\"N\" or @pos=\"PREPDET\" or @pos=\"PREP\"]'):\n",
    "                phrase += w.text.lower() + ' '\n",
    "                #print(w.text, end=' ')\n",
    "            words.append(phrase.strip())\n",
    "            #print()\n",
    "\n",
    "# list to dict with frequency\n",
    "frequency_dict_unknown = {value: words.count(value) for value in words}\n",
    "print('Size of the lexicon', len(frequency_dict_unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files with rs in term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/lmoncla/Documents/Data/Corpus/Visorando/Perdido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for doc in tqdm(sorted(os.listdir(path))):\n",
    "    filename = os.path.join(path, doc+'.xml') \n",
    "    if os.path.exists(filename):\n",
    "        tree = etree.parse(filename)\n",
    "\n",
    "\n",
    "        for term in tree.xpath('.//term//rs'):\n",
    "            print(filename)\n",
    "            phrase = ''\n",
    "            for w in term.xpath('.//w'):\n",
    "                phrase += w.text.lower() + ' '\n",
    "        #print(phrase, end=' ')\n",
    "        #print()\n",
    "            \n",
    "            #print()\n",
    "\n",
    "        # list to dict with frequency\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
